{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "enelpi",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "pwYhUOqHOUwO",
        "outputId": "68a8ae0c-8ead-4239-cff8-ae655b9ffd50"
      },
      "source": [
        "\"\"\"\n",
        "We will train an attention seq2seq for translating from Nahuatl to Spanish, using the Pytorch tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nWe will train an attention seq2seq for translating from Nahuatl to Spanish, using the Pytorch tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGa_x5CkX4I1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810d9b0d-7bec-48c1-f9e2-010b3989b2b4"
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages (0.9.2)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (50.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.5)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIvvsInJicYf"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import time\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "import math\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "# from gensim.models.fasttext import FastTextKeyedVectors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uMcGiBdW7-y",
        "outputId": "fd4e860b-471b-4883-dd04-41b888f12487"
      },
      "source": [
        "fasttext.util.download_model('nah', if_exists='ignore')\n",
        "nh_ft = fasttext.load_model('cc.nah.300.bin')\n",
        "nh_ft.get_dimension()\n",
        "nh_ft.get_nearest_neighbors('coatl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.8674371242523193, 'cuechcoatl'),\n",
              " (0.8571669459342957, 'Olcoatl'),\n",
              " (0.8520733118057251, 'Itzcoatl'),\n",
              " (0.8365020155906677, 'Zolcoatl'),\n",
              " (0.8254373669624329, 'Cuechcoatl'),\n",
              " (0.820296585559845, 'Chiauhcoatl'),\n",
              " (0.8056610822677612, 'Micoatl'),\n",
              " (0.7976199984550476, 'Ocelocoatl'),\n",
              " (0.7928087115287781, 'Roquercoatl'),\n",
              " (0.7875798344612122, 'Coatl')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czb6FI9jAeUO",
        "outputId": "f523f060-5799-4cc4-8a47-aea7cbbe290e"
      },
      "source": [
        "nah_vectors = torch.from_numpy(nh_ft.get_input_matrix())\n",
        "print(nah_vectors.size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2116961, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fj0Xa6YQkeqs"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcmZaHm-RzpA"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I6RCypfSPLk"
      },
      "source": [
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfUZvAcTSSAA"
      },
      "source": [
        "def split_corpus(path):\n",
        "  #Split method thanks to https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
        "  #returns a 60% - 20% -20% training-validation-test split\n",
        "  df = pd.read_csv(path, delimiter=\"|\")\n",
        "  return np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
        "\n",
        "def readLangs(lang1, lang2,corpus, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # lang_df = pd.read_csv(\"parallel_nh-es.csv\", delimiter=\"|\")\n",
        "    # print(lang_df.head())\n",
        "    # print(lang_df.columns.to_list())\n",
        "    pairs = corpus[['Nahuatl', 'Spanish']].to_numpy()\n",
        "    # Normalizes words\n",
        "    pairs = [[normalizeString(y) for y in x] for x in pairs]\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrgflbZtSezS",
        "outputId": "078fdd58-0bcd-48fa-828f-332e05574b02"
      },
      "source": [
        "# May be incresed to get more samples\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH\n",
        "    # and p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "\n",
        "def prepareData(lang1, lang2, corpus,reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2,corpus,reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "training,validation,test = split_corpus(\"parallel_nh-es.csv\")\n",
        "input_lang, output_lang, pairs = prepareData('nh', 'es',training, True)\n",
        "print(random.choice(pairs))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 4692 sentence pairs\n",
            "Trimmed to 1441 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "es 3276\n",
            "nh 3731\n",
            "['y lo ponian a los pies de los apostoles . y era repartido a cada uno segun tenia necesidad', 'niman quinmactiliayaj on apostoles . niman on apostoles quinxelohuiliayaj ocsequimej quen se quipolojticaj .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1k3kKe5Sqwn"
      },
      "source": [
        "\"\"\"Set up the encoder and decoder RNN models!\"\"\"\n",
        "\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size,word_embeds=None):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        if word_embeds is not None:\n",
        "            self.embedding.weight = nn.Parameter(\n",
        "                word_embeds)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLflCqfvSsxf"
      },
      "source": [
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgmPOaF7SvEp"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcpdE_iNSxGo"
      },
      "source": [
        "\"\"\"Prepare the training data.\"\"\"\n",
        "\n",
        "\n",
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ') if word in lang.word2index]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb6jKCDHTAce"
      },
      "source": [
        "\"\"\"\n",
        "Model training code\n",
        "\"\"\"\n",
        "\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH,uses_attn=False):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    # print(\"uses attn\", uses_attn)\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(\n",
        "        max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            if uses_attn:\n",
        "              decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                  decoder_input, decoder_hidden, encoder_outputs)\n",
        "              loss += criterion(decoder_output, target_tensor[di])\n",
        "              decoder_input = target_tensor[di]  # Teacher forcing\n",
        "            # TODO: remove encoder_outputs parameter if not using attention\n",
        "            # TODO: remove decoder_attention from output if not using attention\n",
        "            else:\n",
        "              decoder_output, decoder_hidden = decoder(\n",
        "                  decoder_input, decoder_hidden)\n",
        "              loss += criterion(decoder_output, target_tensor[di])\n",
        "              decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            if uses_attn:\n",
        "              decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                  decoder_input, decoder_hidden, encoder_outputs)\n",
        "              topv, topi = decoder_output.topk(1)\n",
        "              decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "            # TODO: remove encoder_outputs parameter if not using attention\n",
        "            # TODO: remove decoder_attention from output if not using attention\n",
        "            else:\n",
        "              decoder_output, decoder_hidden = decoder(\n",
        "                  decoder_input, decoder_hidden)\n",
        "              topv, topi = decoder_output.topk(1)\n",
        "              decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyVJX82JTQKX"
      },
      "source": [
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnOjmzzgD14W"
      },
      "source": [
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMgjnKLSTXko"
      },
      "source": [
        "\n",
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01,attn=False):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion,uses_attn=attn)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEc_oIXBTcZn"
      },
      "source": [
        "\"\"\"\n",
        "Evaluation\"\"\"\n",
        "\n",
        "\n",
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(\n",
        "            max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            try:\n",
        "              decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                  decoder_input, decoder_hidden, encoder_outputs)\n",
        "              decoder_attentions[di] = decoder_attention.data\n",
        "            except:\n",
        "              decoder_output, decoder_hidden = decoder(\n",
        "                  decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "MzOrxp1VTdUj",
        "outputId": "e40f62f0-f714-4a7f-82fd-f831137f0aba"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder,pairs, n=1):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')\n",
        "\n",
        "\n",
        "\"\"\"Calculating the BLEU score on a subset of the test data\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Calculating the BLEU score on a subset of the test data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwQF_ymQTfkh"
      },
      "source": [
        "def evaluate_bleu(encoder, decoder,pairs, n=10):\n",
        "    \"\"\"To speed up testing, we only evaluate BLEU score on n test sentences.\"\"\"\n",
        "    references = []\n",
        "    predictions = []\n",
        "    for pair in pairs[:n]:\n",
        "        references.append(pair[1])\n",
        "        output_words, _ = evaluate(encoder, decoder, pair[0])\n",
        "        predictions.append(output_words)\n",
        "    #smoothing function reference here: https://www.nltk.org/_modules/nltk/translate/bleu_score.html\n",
        "    score = corpus_bleu(references, predictions,smoothing_function=SmoothingFunction().method3)\n",
        "    print('BLEU score:', score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efWdG2kpTijw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4c50e6-97df-4b32-ba04-9557b5d408ce"
      },
      "source": [
        "\n",
        "\"\"\"Finally, we can actually run and test the model (without attention)!\"\"\"\n",
        "# TODO test this once everything else works\n",
        "hidden_size = 300\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "encoder2 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "# TODO: try DecoderRNN without attention (remove dropout_p parameter)\n",
        "decoder1 = DecoderRNN(hidden_size,output_lang.n_words).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(\n",
        "    hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "# TO speed up training, you can reduce 75000 to 5000\n",
        "trainIters(encoder1, decoder1, 5000, print_every=5000)\n",
        "trainIters(encoder2, attn_decoder1, 5000, print_every=5000,attn=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12m 29s (- 0m 0s) (5000 100%) 4.8099\n",
            "14m 7s (- 0m 0s) (5000 100%) 4.7873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4S00KggVXv3",
        "outputId": "25bd2454-e959-4d86-92d6-f1f1e14f5c4d"
      },
      "source": [
        "_,_,val_pairs = prepareData('nh', 'es',validation, True)\n",
        "_,_,test_pairs = prepareData('nh', 'es',test, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "Read 1564 sentence pairs\n",
            "Trimmed to 510 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "es 1692\n",
            "nh 1832\n",
            "Reading lines...\n",
            "Read 1565 sentence pairs\n",
            "Trimmed to 465 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "es 1549\n",
            "nh 1720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0rwF0OpFXOu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d23bb4-8a4a-4a3b-97c5-2796811e2aae"
      },
      "source": [
        "# evaluateRandomly(encoder1, attn_decoder1,val_pairs)\n",
        "evaluate_bleu(encoder1, decoder1,val_pairs,n=len(val_pairs))\n",
        "evaluate_bleu(encoder2,attn_decoder1,val_pairs,n=len(val_pairs))\n",
        "evaluate_bleu(encoder1,attn_decoder1,val_pairs,n=len(val_pairs))\n",
        "evaluate_bleu(encoder2,decoder1,val_pairs,n=len(val_pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU score: 0.0006377897438035261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz9-uVuoTLeb"
      },
      "source": [
        "def save_translator(encoder,decoder,path1=\"encoder\",path2=\"decoder\"):\n",
        "  torch.save(encoder,path1)\n",
        "  torch.save(decoder,path2)\n",
        "\n",
        "def load_translator(encoder_path,decoder_path):\n",
        "  return torch.load(encoder_path) , torch.load(decoder_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXoiNaFRVBIG"
      },
      "source": [
        "def evaluate_saved(encoder_path,decoder_path,pairs):\n",
        "  evaluate_bleu(*load_translator(encoder_path,decoder_path),pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arJhpqRFV1za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "750b2f58-c3db-4a58-f357-18a44632cdc4"
      },
      "source": [
        "# save_translator(encoder1,attn_decoder1)\n",
        "# save_translator(encoder1,attn_decoder1,path1=\"/content/drive/MyDrive/NLP_project/encoder1\",path2=\"/content/drive/MyDrive/NLP_project/decoder1\")\n",
        "evaluate_saved(\"encoder\",\"decoder\",val_pairs)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU score: 0.010107123741942224\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}